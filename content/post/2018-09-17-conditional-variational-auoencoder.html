---
title: 条件变分自动编码器（Conditional variational auoencoder）
author: ~
date: '2018-09-17'
slug: conditional-variational-auoencoder
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    number_sections: true
---


<div id="TOC">
<ul>
<li><a><span class="toc-section-number">1</span> 概述</a></li>
<li><a href="#getting-started"><span class="toc-section-number">2</span> Getting started</a><ul>
<li><a href="#a-small-example"><span class="toc-section-number">2.1</span> A small example</a></li>
<li><a href="#variational-autoencoder"><span class="toc-section-number">2.2</span> variational autoencoder</a></li>
</ul></li>
</ul>
</div>

<div class="section level1">
<h1><span class="header-section-number">1</span> 概述</h1>
<p><em>keras</em> 作为深度学习的框架之一，之前一直用 <em>python</em> 来写 <em>keras</em> 的代码，但是<a href="https://keras.rstudio.com">R语言支持keras</a>，有一个想法就是，利用条件自动编码器来生成数据用于训练，所以用 <em>R</em> 中 <em>keras</em> 来编写一下模型。</p>
</div>
<div id="getting-started" class="section level1">
<h1><span class="header-section-number">2</span> Getting started</h1>
<p>安装 <em>keras</em> 的包</p>
<pre><code>install.packages(&quot;keras&quot;)</code></pre>
<p><em>keras</em> 默认使用 <em>tensorflow</em> 作为后台，如果需要更改，请参见<code>install_keras()</code>的帮助</p>
<pre class="r"><code>library(keras)</code></pre>
<pre><code>## Warning: 程辑包&#39;keras&#39;是用R版本3.4.4 来建造的</code></pre>
<pre class="r"><code>install_keras()</code></pre>
<pre><code>## Creating virtualenv for TensorFlow at  ~/.virtualenvs/r-tensorflow 
## Installing TensorFlow ...
## 
## Installation complete.</code></pre>
<div id="a-small-example" class="section level2">
<h2><span class="header-section-number">2.1</span> A small example</h2>
<pre class="r"><code># prepare data
library(keras)
mnist &lt;- dataset_mnist()
x_train &lt;- mnist$train$x
y_train &lt;- mnist$train$y
x_test &lt;- mnist$test$x
y_test &lt;- mnist$test$y

# reshape
x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))
x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train &lt;- x_train / 255
x_test &lt;- x_test / 255

y_train &lt;- to_categorical(y_train, 10)
y_test &lt;- to_categorical(y_test, 10)

# define the model
model &lt;- keras_model_sequential() 
model %&gt;% 
  layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&gt;% 
  layer_dropout(rate = 0.4) %&gt;% 
  layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;%
  layer_dropout(rate = 0.3) %&gt;%
  layer_dense(units = 10, activation = &#39;softmax&#39;)

summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 256)                   200960      
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 256)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 128)                   32896       
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 128)                   0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 10)                    1290        
## ===========================================================================
## Total params: 235,146
## Trainable params: 235,146
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p><strong>model compile</strong></p>
<pre class="r"><code>model %&gt;% compile(
  loss = &#39;categorical_crossentropy&#39;,
  optimizer = optimizer_rmsprop(),
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
<p><strong>model train</strong></p>
<pre class="r"><code>history &lt;- model %&gt;% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)</code></pre>
<p><strong>plot training process</strong></p>
<pre class="r"><code>plot(history)</code></pre>
<p><img src="/post/2018-09-17-conditional-variational-auoencoder_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><strong>model evaluate</strong></p>
<pre class="r"><code>model %&gt;% evaluate(x_test, y_test)</code></pre>
<pre><code>## $loss
## [1] 0.1019124
## 
## $acc
## [1] 0.9812</code></pre>
<p><strong>model prediction</strong></p>
<pre><code>model %&gt;% predict_classes(x_test)</code></pre>
</div>
<div id="variational-autoencoder" class="section level2">
<h2><span class="header-section-number">2.2</span> variational autoencoder</h2>
<pre class="r"><code>library(keras)
K &lt;- keras::backend()

# Pamameters

batch_size &lt;- 100L
original_dim &lt;- 784L
latent_dim &lt;- 2L
intermediate_dim &lt;- 256L
epochs &lt;- 50L
epsilon_std &lt;- 1.0

# construct network
x &lt;- layer_input(shape = c(original_dim))
h &lt;- layer_dense(x, intermediate_dim, activation = &#39;relu&#39;)
z_mean &lt;- layer_dense(h,latent_dim)
z_log_var &lt;- layer_dense(h,latent_dim)

sampling &lt;- function(arg){
  z_mean &lt;- arg[,1:(latent_dim)]
  z_log_var &lt;- arg[,(latent_dim + 1):(2 * latent_dim)]
  
  epsilon &lt;- k_random_normal(shape = c(k_shape(z_mean)[[1]]),
                             mean = 0.,
                             stddev = epsilon_std)
  z_mean + k_exp(z_log_var/2)*epsilon
}

z &lt;- layer_concatenate(list(z_mean,z_log_var)) %&gt;%
  layer_lambda(sampling)

decoder_h &lt;- layer_dense(units = intermediate_dim, activation = &#39;relu&#39;)
decoder_mean &lt;- layer_dense(units = original_dim, activation = &#39;sigmoid&#39;)

h_decoded &lt;- decoder_h(z)
x_decoded_mean &lt;- decoder_mean(h_decoded)

vae &lt;- keras_model(x,x_decoded_mean)

encoder &lt;- keras_model(x,z_mean)

decoder_input &lt;- layer_input(shape = latent_dim)
h_decoded_2 &lt;- decoder_h(decoder_input)
x_decoded_mean_2 &lt;- decoder_mean(h_decoded_2)
generator &lt;- keras_model(decoder_input,x_decoded_mean_2)

# define loss function
vae_loss &lt;- function(x,x_decoded_mean){
  xent_loss &lt;- (original_dim/1.0)*loss_binary_crossentropy(x,x_decoded_mean)
  kl_loss &lt;- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var),axis = -1L)
  xent_loss + kl_loss
}

vae %&gt;% compile(optimizer = &#39;rmsprop&#39;, loss = vae_loss)

# Data preparation
mnist &lt;- dataset_mnist()
x_train &lt;- mnist$train$x/255
x_test &lt;- mnist$test$x/255
x_train &lt;- x_train %&gt;% apply(1, as.numeric) %&gt;% t()
x_test &lt;- x_test %&gt;% apply(1, as.numeric) %&gt;% t()

# model training
vae %&gt;% fit(
  x_train,x_train,
  shuffle = TRUE,
  epochs = epochs,
  batch_size = batch_size,
  validation_data = list(x_test, x_test)
)

# visualization
library(ggplot2)</code></pre>
<pre><code>## Warning: 程辑包&#39;ggplot2&#39;是用R版本3.4.4 来建造的</code></pre>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## Warning: 程辑包&#39;dplyr&#39;是用R版本3.4.4 来建造的</code></pre>
<pre><code>## 
## 载入程辑包：&#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>x_test_encoded &lt;- predict(encoder, x_test, batch_size = batch_size)

# x distribution at feature space
x_test_encoded %&gt;% as_data_frame() %&gt;%
  mutate(class = as.factor(mnist$test$y)) %&gt;%
  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()</code></pre>
<pre><code>## Warning: 程辑包&#39;bindrcpp&#39;是用R版本3.4.4 来建造的</code></pre>
<p><img src="/post/2018-09-17-conditional-variational-auoencoder_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>n &lt;- 15
digit_size &lt;- 28

grid_x &lt;- seq(-4,4, length.out = n)
grid_y &lt;- seq(-4,4, length.out = n)

rows &lt;- NULL
for(i in 1:length(grid_x)){
  column &lt;- NULL
  for(j in 1:length(grid_y)){
    z_sample &lt;- matrix(c(grid_x[i],grid_y[j]), ncol = 2)
    column &lt;- rbind(column, predict(generator, z_sample) %&gt;% matrix(ncol = 28))
  }
  rows &lt;- cbind(rows, column)
}

rows %&gt;% as.raster() %&gt;% plot()</code></pre>
<p><img src="/post/2018-09-17-conditional-variational-auoencoder_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
</div>
</div>
