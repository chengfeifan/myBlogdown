---
title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding论文学习'
author: ~
date: '2018-10-15'
slug: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
categories: [deep learning]
tags: [language]
---

# 摘要
本文提出了一个新的语言表达方式叫做 *BERT* ，代表着Bidirectional Encoder Representations from Transformers。不想其他的语言表达模型，BERT 就是为预处理深度双向表达设计的。因此，BERT可以通过增加一层输出层来实现多种任务，例如问题回答和语言推断，不需要专门针对一个任务来改进结构。BERT是概念上简单以及非常有效，在11个自然语言问题处理很有效果。

# 正文

