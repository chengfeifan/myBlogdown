---
title: 条件变分自动编码器（Conditional variational auoencoder）
author: ~
date: '2018-09-17'
slug: conditional-variational-auoencoder
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    number_sections: true
---

# 概述
*keras* 作为深度学习的框架之一，之前一直用 *python* 来写 *keras* 的代码，但是[R语言支持keras](https://keras.rstudio.com)，有一个想法就是，利用条件自动编码器来生成数据用于训练，所以用 *R* 中 *keras* 来编写一下模型。

# Getting started
安装 *keras* 的包

```
install.packages("keras")
```
*keras* 默认使用 *tensorflow* 作为后台，如果需要更改，请参见`install_keras()`的帮助

```{r}
library(keras)
install_keras()
```

## A small example

```{r}
# prepare data
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

# define the model
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model)
```

**model compile**

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

**model train**

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

**plot training process**

```{r}
plot(history)
```

**model evaluate**

```{r}
model %>% evaluate(x_test, y_test)
```

**model prediction**

```
model %>% predict_classes(x_test)
```

## variational autoencoder

```{r}
library(keras)
K <- keras::backend()

# Pamameters

batch_size <- 100L
original_dim <- 784L
latent_dim <- 2L
intermediate_dim <- 256L
epochs <- 50L
epsilon_std <- 1.0

# construct network
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = 'relu')
z_mean <- layer_dense(h,latent_dim)
z_log_var <- layer_dense(h,latent_dim)

sampling <- function(arg){
  z_mean <- arg[,1:(latent_dim)]
  z_log_var <- arg[,(latent_dim + 1):(2 * latent_dim)]
  
  epsilon <- k_random_normal(shape = c(k_shape(z_mean)[[1]]),
                             mean = 0.,
                             stddev = epsilon_std)
  z_mean + k_exp(z_log_var/2)*epsilon
}

z <- layer_concatenate(list(z_mean,z_log_var)) %>%
  layer_lambda(sampling)

decoder_h <- layer_dense(units = intermediate_dim, activation = 'relu')
decoder_mean <- layer_dense(units = original_dim, activation = 'sigmoid')

h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)

vae <- keras_model(x,x_decoded_mean)

encoder <- keras_model(x,z_mean)

decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input,x_decoded_mean_2)

# define loss function
vae_loss <- function(x,x_decoded_mean){
  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x,x_decoded_mean)
  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var),axis = -1L)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = 'rmsprop', loss = vae_loss)

# Data preparation
mnist <- dataset_mnist()
x_train <- mnist$train$x/255
x_test <- mnist$test$x/255
x_train <- x_train %>% apply(1, as.numeric) %>% t()
x_test <- x_test %>% apply(1, as.numeric) %>% t()

# model training
vae %>% fit(
  x_train,x_train,
  shuffle = TRUE,
  epochs = epochs,
  batch_size = batch_size,
  validation_data = list(x_test, x_test)
)

# visualization
library(ggplot2)
library(dplyr)
x_test_encoded <- predict(encoder, x_test, batch_size = batch_size)

# x distribution at feature space
x_test_encoded %>% as_data_frame() %>%
  mutate(class = as.factor(mnist$test$y)) %>%
  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()

n <- 15
digit_size <- 28

grid_x <- seq(-4,4, length.out = n)
grid_y <- seq(-4,4, length.out = n)

rows <- NULL
for(i in 1:length(grid_x)){
  column <- NULL
  for(j in 1:length(grid_y)){
    z_sample <- matrix(c(grid_x[i],grid_y[j]), ncol = 2)
    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = 28))
  }
  rows <- cbind(rows, column)
}

rows %>% as.raster() %>% plot()
```


