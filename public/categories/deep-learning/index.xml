<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Blog of Feifan Cheng</title>
    <link>/categories/deep-learning/</link>
    <description>Recent content in Deep Learning on Blog of Feifan Cheng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding论文学习</title>
      <link>/2018/10/15/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/15/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</guid>
      <description> 摘要 本文提出了一个新的语言表达方式叫做 BERT ，代表着Bidirectional Encoder Representations from Transformers。不想其他的语言表达模型，BERT 就是为预处理深度双向表达设计的。因此，BERT可以通过增加一层输出层来实现多种任务，例如问题回答和语言推断，不需要专门针对一个任务来改进结构。BERT是概念上简单以及非常有效，在11个自然语言问题处理很有效果。
正文 </description>
    </item>
    
  </channel>
</rss>